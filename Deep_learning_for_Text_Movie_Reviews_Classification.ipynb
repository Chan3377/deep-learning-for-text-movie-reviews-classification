{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0i0HcvEVGGx"
      },
      "source": [
        "# Deep learning for text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3Ulh-iUVGG0"
      },
      "source": [
        "## Preparing text data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Vectorizing text is the process of transforming text into numeric tensors.\n",
        "- Text vectorization processes come in many shapes and forms, but they all follow the same template:\n",
        "    - First, we ***standardize the text*** to make it easier to process, such as by ***converting it to lowercase or removing punctuation***.\n",
        "    - we ***split the text into units (called tokens)***, such as characters, words, or groups of words. This is called ***tokenization***.\n",
        "    - we ***convert each such token into a numerical vector***. This will usually involve first indexing all tokens present in the data."
      ],
      "metadata": {
        "id": "OjFtv7dIPNgG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9m1V0CkxVGG0"
      },
      "source": [
        "### Text standardization"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Text standardization is a basic form of feature engineering that aims to erase encoding differences that you don’t want your model to have to deal with.\n",
        "- One of the simplest and most widespread standardization schemes is “convert to lowercase and remove punctuation characters.”\n",
        "- Another common transformation is to convert special characters to a standard form, such as ***replacing “é” with “e,” “æ” with “ae,”***.\n",
        "- A much more advanced standardization pattern that is more rarely used in a machine learning context is stemming: converting variations of a term (such as different conjugated forms of a verb) into a single shared representation, like turning ***“caught” and “been catching” into “[catch]”*** or ***“cats” into “[cat]”***. With stemming, “was staring” and ***“stared” would become something like “[stare]”***, and our two similar sentences would finally end up with an identical encoding:\n",
        "    - “sunset came i [stare] at the mexico sky isnt nature splendid”"
      ],
      "metadata": {
        "id": "KyYIlwlnQXCg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9e6WqY6VGG1"
      },
      "source": [
        "### Text splitting (tokenization)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Once your text is standardized, you need to break it up into units to be vectorized (tokens), a step called ***tokenization***\n",
        "    - ***Word-level tokenization***—Where tokens are space-separated (or punctuation separated) substrings. A variant of this is to further split words into subwords when applicable—for instance, treating “staring” as “star+ing” or “called” as “call+ed.”\n",
        "    - ***N-gram tokenization***—Where tokens are groups of N consecutive words. For instance, “the cat” or “he was” would be 2-gram tokens (also called bigrams).\n",
        "    - ***Character-level tokenization***—Where each character is its own token. In practice, this scheme is rarely used, and you only really see it in specialized contexts, like text generation or speech recognition.\n",
        "- There are two kinds of text-processing models:\n",
        "    - those that ***care about word order***, called ***sequence models***, and\n",
        "    - those that ***treat input words as a set, discarding their original order***, called ***bag-of-words* models**.\n",
        "- If you’re building a ***sequence model***, you’ll use ***word-level tokenization***,\n",
        "- If you’re building a ***bag-of-words model***, you’ll use ***N-gram tokenization***.\n",
        "    - ***N-grams*** are a way to artificially inject a small amount of local word order information into the model.\n",
        "    - The term ***“bag”*** here refers to the fact that you’re dealing with a ***set of tokens*** rather than a list or sequence: the tokens have no specific order.\n",
        "    - ***Bag-of-words*** isn’t an order-preserving tokenization method (the tokens generated are understood *as a set, not a sequence*, and the general structure of the sentences is lost)."
      ],
      "metadata": {
        "id": "fOZyAPj_Qb-p"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XJqEeoyVGG2"
      },
      "source": [
        "### Vocabulary indexing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Once your text is split into tokens, you need to ***encode each token into a numerical representation***\n",
        "- **Important detail**:\n",
        "    - When we look up a new token in our vocabulary index, it may not necessarily exist. Your training data may not have contained any instance of the word “cherimoya” (or maybe you excluded it from your index because it was too rare), so ***doing token_index = vocabulary[\"cherimoya\"] may result in a KeyError***.\n",
        "    - To handle this, you should use an ***“out of vocabulary” index (abbreviated as OOV index)***—*a catch-all for any token that wasn’t in the index*.\n",
        "        - It’s usually index 1: you’re actually doing ***token_index = vocabulary.get(token, 1)***.\n",
        "        - When ***decoding a sequence of integers back into words***, you’ll ***replace 1 with something like “[UNK]” (which you’d call an “OOV token”)***.\n",
        "        - While the ***OOV token means*** “here was a word we did not recognize,” the mask token tells us “ignore me, I’m not a word.”\n",
        "        - Use it in particular to ***pad sequence data***: because data batches need to be ***contiguous***,\n",
        "            - all sequences in a batch of sequence data must have the same length,\n",
        "            - so shorter sequences should be padded to the length of the longest sequence.\n",
        "            - If you want to make a batch of data with the sequences [5, 7, 124, 4, 89] and [8, 34, 21], it would have to look like this:\n",
        "                - [[5, 7, 124, 4, 89]\n",
        "                [8, 34, 21, 0, 0]]"
      ],
      "metadata": {
        "id": "tL_sxK15RDgh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O09X16BEVGG2"
      },
      "source": [
        "### Using the TextVectorization layer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write code vectorizer in pure python"
      ],
      "metadata": {
        "id": "bmY6rJ4XRMwM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "o_iFcOl0VGG3"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "class Vectorizer:\n",
        "    def standardize(self, text):\n",
        "        text = text.lower()\n",
        "        return \"\".join(char for char in text if char not in string.punctuation)\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        text = self.standardize(text)\n",
        "        return text.split()\n",
        "\n",
        "    def make_vocabulary(self, dataset):\n",
        "        self.vocabulary = {\"\": 0, \"[UNK]\": 1}\n",
        "        for text in dataset:\n",
        "            text = self.standardize(text)\n",
        "            tokens = self.tokenize(text)\n",
        "            for token in tokens:\n",
        "                if token not in self.vocabulary:\n",
        "                    self.vocabulary[token] = len(self.vocabulary)\n",
        "        self.inverse_vocabulary = dict(\n",
        "            (v, k) for k, v in self.vocabulary.items())\n",
        "\n",
        "    def encode(self, text):\n",
        "        text = self.standardize(text)\n",
        "        tokens = self.tokenize(text)\n",
        "        return [self.vocabulary.get(token, 1) for token in tokens]\n",
        "\n",
        "    def decode(self, int_sequence):\n",
        "        return \" \".join(\n",
        "            self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence)\n",
        "\n",
        "vectorizer = Vectorizer()\n",
        "dataset = [\n",
        "    \"I know, understand, comprehend\",\n",
        "    \"forget again, and then\",\n",
        "    \"A buliding blooms.\",\n",
        "]\n",
        "vectorizer.make_vocabulary(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NacVECOMVGG8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62e6741f-ffc5-4c0c-f7cd-3c6f80dadaf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 3, 4, 5, 8, 1, 6, 7]\n"
          ]
        }
      ],
      "source": [
        "test_sentence = \"I know, understand, comprehend, and still forget again\"\n",
        "encoded_sentence = vectorizer.encode(test_sentence)\n",
        "print(encoded_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XheNmnQBVGG-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78667aae-c7e2-4592-836b-d8d7879db6a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i know understand comprehend and [UNK] forget again\n"
          ]
        }
      ],
      "source": [
        "decoded_sentence = vectorizer.decode(encoded_sentence)\n",
        "print(decoded_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KIOKNoAzVGHA"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "text_vectorization = TextVectorization(\n",
        "    # Configures the layer to return sequences of words encoded as integer indices.\n",
        "    output_mode=\"int\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "CEqRDAiWVGHC"
      },
      "outputs": [],
      "source": [
        "# the default keras TextVectorization layer behavior is equivalent to python code below\n",
        "import re\n",
        "import string\n",
        "import tensorflow as tf\n",
        "\n",
        "def custom_standardization_fn(string_tensor):\n",
        "    # convert strings to lowercase\n",
        "    lowercase_string = tf.strings.lower(string_tensor)\n",
        "    # replace punctuation characters with the rmpty string\n",
        "    return tf.strings.regex_replace(\n",
        "        lowercase_string, f\"[{re.escape(string.punctuation)}]\", \"\")\n",
        "\n",
        "def custom_split_fn(string_tensor):\n",
        "    # split strings on whitespace\n",
        "    return tf.strings.split(string_tensor)\n",
        "\n",
        "text_vectorization = TextVectorization(\n",
        "    output_mode=\"int\",\n",
        "    standardize=custom_standardization_fn,\n",
        "    split=custom_split_fn,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HEErC4k_VGHE"
      },
      "outputs": [],
      "source": [
        "dataset = [\n",
        "    \"I know, understand, comprehend\",\n",
        "    \"forget again, and then\",\n",
        "    \"A buliding blooms.\",\n",
        "]\n",
        "# adapt() method - to index the vocabulary of a text corpus\n",
        "text_vectorization.adapt(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LCE99piVGHF"
      },
      "source": [
        "**Displaying the vocabulary**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ZCJkeOoKVGHF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa85ff27-b450-4eac-b09f-4006e78419bf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '[UNK]',\n",
              " 'understand',\n",
              " 'then',\n",
              " 'know',\n",
              " 'i',\n",
              " 'forget',\n",
              " 'comprehend',\n",
              " 'buliding',\n",
              " 'blooms',\n",
              " 'and',\n",
              " 'again',\n",
              " 'a']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "text_vectorization.get_vocabulary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xB4mN8-pVGHG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cf0aa31-6645-41e7-b851-85a90d692cd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([ 5  4  2  7 10  1  6 11], shape=(8,), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "# Retrieve the computed vocabulary via get_vocabulary()—this can be useful\n",
        "# if you need to convert text encoded as integer sequences back into words\n",
        "vocabulary = text_vectorization.get_vocabulary()\n",
        "test_sentence = \"I know, understand, comprehend, and still forget again\"\n",
        "encoded_sentence = text_vectorization(test_sentence)\n",
        "print(encoded_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8uvhDiDpVGHH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74b57947-1cf6-46b1-ff63-433c86305f35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i know understand comprehend and [UNK] forget again\n"
          ]
        }
      ],
      "source": [
        "inverse_vocab = dict(enumerate(vocabulary))\n",
        "decoded_sentence = \" \".join(inverse_vocab[int(i)] for i in encoded_sentence)\n",
        "print(decoded_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTUxGugiVGHI"
      },
      "source": [
        "## Two approaches for representing groups of words: Sets and sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZ2T11ZKVGHI"
      },
      "source": [
        "### Preparing the IMDB movie reviews data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "xdIMGA5FVGHJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "736c00f0-1eac-4763-bde5-85597eada974"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  5788k      0  0:00:14  0:00:14 --:--:-- 12.1M\n"
          ]
        }
      ],
      "source": [
        "# download the file\n",
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "P2uIczbWVGHJ"
      },
      "outputs": [],
      "source": [
        "# No need this file - delete this file\n",
        "!rm -r aclImdb/train/unsup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "mm6K7UONVGHK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d97af78-c19a-45b8-8187-fd6c599cd8a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy"
          ]
        }
      ],
      "source": [
        "# Take a look at the content of a few of these text files\n",
        "!cat aclImdb/train/pos/4077_10.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "zTkjkmOJVGHK"
      },
      "outputs": [],
      "source": [
        "import os, pathlib, shutil, random\n",
        "\n",
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\"\n",
        "for category in (\"neg\", \"pos\"):\n",
        "    os.makedirs(val_dir / category)\n",
        "    files = os.listdir(train_dir / category)\n",
        "    # Shuffle the list of training files\n",
        "    random.Random(1337).shuffle(files)\n",
        "    # Take 20% of the training files to use for validation\n",
        "    num_val_samples = int(0.2 * len(files))\n",
        "    val_files = files[-num_val_samples:]\n",
        "    # Move the files to aclImdb/val/neg and aclImdb/val/pos.\n",
        "    for fname in val_files:\n",
        "        shutil.move(train_dir / category / fname,\n",
        "                    val_dir / category / fname)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "kZLJ-uhrVGHL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16edd320-babb-4c83-f9f0-86d80f8e4d9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20000 files belonging to 2 classes.\n",
            "Found 5000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "batch_size = 32\n",
        "\n",
        "# Running this line should output “Found 20000 files belonging to 2 classes”\n",
        "train_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\", batch_size=batch_size\n",
        ")\n",
        "val_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/val\", batch_size=batch_size\n",
        ")\n",
        "test_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phZmHCA7VGHL"
      },
      "source": [
        "**Displaying the shapes and dtypes of the first batch**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "imVExuzjVGHL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d81f6b3-cdc1-4b0b-c42d-7cebd8e64208"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs.shape: (32,)\n",
            "inputs.dtype: <dtype: 'string'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0]: tf.Tensor(b'Dr. McCoy and Mr. Spock find themselves trapped in a planet\\'s past Ice Age, while Capt. Kirk is in the same planet\\'s colonial period. However, it\\'s the former pair that has the most trying time. Besides the freezing temperatures and sanctuary to be found only in caves, there is a third inhabitant, the beautiful and so sexy Zarabeth (Mariette Hartley). As Spock spends more time in this era, he slowly begins to revert to the behavioral patterns of his ancestors, feeling a natural attraction to Zarabeth and throwing \"caution to the wind\" about ever leaving this place. Only with Dr. McCoy\\'s constant \"reminders\" does Spock hold on to some grasp of reality.<br /><br />This stand as one of the few times when the character gets to show some \"emotion\" and Nimoy (Spock) plays it to the hilt, coming close to knocking the bejesus out of Deforest Kelly (McCoy). Surprising to previous installment, Captain Kirk (William Shatner) wasn\\'t allowed to get the girl, another plus for this one.<br /><br />Perennial \"old man\" Ian Wolfe assays the role of \"Mr. Atoz,\" the librarian responsible for sending the trio into the past.', shape=(), dtype=string)\n",
            "targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "for inputs, targets in train_ds:\n",
        "    print(\"inputs.shape:\", inputs.shape)\n",
        "    print(\"inputs.dtype:\", inputs.dtype)\n",
        "    print(\"targets.shape:\", targets.shape)\n",
        "    print(\"targets.dtype:\", targets.dtype)\n",
        "    print(\"inputs[0]:\", inputs[0])\n",
        "    print(\"targets[0]:\", targets[0])\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUnycRpFVGHM"
      },
      "source": [
        "### Processing words as a set: The bag-of-words approach"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The simplest way to encode a piece of text for processing by a machine learning model is to discard order and treat it as a set (a “bag”) of tokens\n",
        "    - Could either look at individual words (unigrams),\n",
        "    - or try to Recover some ***local order information*** by looking at groups of consecutive token (N-grams)."
      ],
      "metadata": {
        "id": "8GQmr830UoF1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOrjkg74VGHN"
      },
      "source": [
        "#### Single words (unigrams) with binary encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The main advantage of this encoding is that you can represent an entire text as a single vector, where each entry is a presence indicator for a given word."
      ],
      "metadata": {
        "id": "_Y6ziC3dUq_y"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_W5pgkOlVGHN"
      },
      "source": [
        "**Preprocessing our datasets with a `TextVectorization` layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "_Do9rc_5VGHN"
      },
      "outputs": [],
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    # Limit the vocabulary to the 20,000 most frequent words\n",
        "    # In general, 20,000 is the right vocabulary size for text classification\n",
        "    max_tokens=20000,\n",
        "    # Encode the output tokens as multi-hot binary vectors\n",
        "    output_mode=\"multi_hot\",\n",
        ")\n",
        "\n",
        "# Prepare a dataset that only yields raw text inputs (no labels).\n",
        "# (lambda x, y: x) - lambda function that takes two arguments, x and y, and returns x(inputs).\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
        "# Use that dataset to index the dataset vocabulary via the adapt() method\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "# Prepare processed versions of our training, validation, and test dataset.\n",
        "# Specify num_parallel_calls to leverage multiple CPU cores.\n",
        "binary_1gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_1gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_1gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5DnCbFmVGHO"
      },
      "source": [
        "**Inspecting the output of our binary unigram dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "EGVa2fv8VGHP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "661f80ca-e5f1-407d-942e-f9832aec115e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs.shape: (32, 20000)\n",
            "inputs.dtype: <dtype: 'float32'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0]: tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n",
            "targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "for inputs, targets in binary_1gram_train_ds:\n",
        "    print(\"inputs.shape:\", inputs.shape)\n",
        "    print(\"inputs.dtype:\", inputs.dtype)\n",
        "    print(\"targets.shape:\", targets.shape)\n",
        "    print(\"targets.dtype:\", targets.dtype)\n",
        "    print(\"inputs[0]:\", inputs[0])\n",
        "    print(\"targets[0]:\", targets[0])\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1DUsKrYVGHQ"
      },
      "source": [
        "**Our model-building utility**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "_Ly2IDjVVGHR"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def get_model(max_tokens=20000, hidden_dim=16):\n",
        "    inputs = keras.Input(shape=(max_tokens,))\n",
        "    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    model.compile(optimizer=\"rmsprop\",\n",
        "                  loss=\"binary_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlK7gsMAVGHS"
      },
      "source": [
        "**Training and testing the binary unigram model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "gNu5qikcVGHS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46e35f8f-69fe-4c67-9162-e19298be0c3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 16)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320033 (1.22 MB)\n",
            "Trainable params: 320033 (1.22 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 9s 9ms/step - loss: 0.4230 - accuracy: 0.8184 - val_loss: 0.2953 - val_accuracy: 0.8818\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2714 - accuracy: 0.8967 - val_loss: 0.2934 - val_accuracy: 0.8854\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2434 - accuracy: 0.9146 - val_loss: 0.2832 - val_accuracy: 0.8974\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2275 - accuracy: 0.9222 - val_loss: 0.3002 - val_accuracy: 0.8944\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2233 - accuracy: 0.9280 - val_loss: 0.3089 - val_accuracy: 0.8898\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2111 - accuracy: 0.9305 - val_loss: 0.3190 - val_accuracy: 0.8944\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2053 - accuracy: 0.9338 - val_loss: 0.3307 - val_accuracy: 0.8892\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2072 - accuracy: 0.9348 - val_loss: 0.3382 - val_accuracy: 0.8864\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2051 - accuracy: 0.9367 - val_loss: 0.3441 - val_accuracy: 0.8896\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2050 - accuracy: 0.9370 - val_loss: 0.3541 - val_accuracy: 0.8834\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.3022 - accuracy: 0.8858\n",
            "Test acc: 0.886\n"
          ]
        }
      ],
      "source": [
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"binary_1gram.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(binary_1gram_train_ds.cache(),\n",
        "          validation_data=binary_1gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "model = keras.models.load_model(\"binary_1gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")\n",
        "\n",
        "# cache()- We call cache() on the datasets to cache them in memory:\n",
        "# this way, we will only do the preprocessing once, during the first epoch,\n",
        "# and we’ll reuse the preprocessed texts for the following epochs.\n",
        "# This can only be done if the data is small enough to fit in memory."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This gets a test accuracy of 88.6%"
      ],
      "metadata": {
        "id": "CX53QKJfVEHy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62YRjibpVGHT"
      },
      "source": [
        "#### Bigrams with binary encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Discarding (get rid of) word order, the concepts can be expressed via multiple word that is quite distinct from the meaning of the words taken separately. The term **“United States”** can be **“states” and “united”**\n",
        "- For this reason, you will usually end up re-injecting local order information into the bag-of-words representation by looking at N-grams rather than single words (most commonly, bigrams)."
      ],
      "metadata": {
        "id": "ibMN3orVlsQ3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNLBeV0vVGHT"
      },
      "source": [
        "**Configuring the `TextVectorization` layer to return bigrams**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The **TextVectorization** layer can be configured to return arbitrary N-grams: bigrams, trigrams, etc.\n",
        "- Just pass an **ngrams=N** argument"
      ],
      "metadata": {
        "id": "QJVDcm3AlO_7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "RscrIhfuVGHV"
      },
      "outputs": [],
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    ngrams=2,\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"multi_hot\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQiHiQnGVGHW"
      },
      "source": [
        "**Training and testing the binary bigram model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "QaPAYpL0VGHW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb82064d-faa0-415f-87ec-b8b7ff441bf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320033 (1.22 MB)\n",
            "Trainable params: 320033 (1.22 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 7s 10ms/step - loss: 0.3939 - accuracy: 0.8346 - val_loss: 0.2630 - val_accuracy: 0.8998\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2499 - accuracy: 0.9082 - val_loss: 0.2602 - val_accuracy: 0.9020\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2129 - accuracy: 0.9278 - val_loss: 0.2767 - val_accuracy: 0.9026\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.1882 - accuracy: 0.9390 - val_loss: 0.2943 - val_accuracy: 0.9004\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.1728 - accuracy: 0.9449 - val_loss: 0.3164 - val_accuracy: 0.9026\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.1726 - accuracy: 0.9479 - val_loss: 0.3284 - val_accuracy: 0.8962\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.1675 - accuracy: 0.9502 - val_loss: 0.3432 - val_accuracy: 0.9018\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.1652 - accuracy: 0.9527 - val_loss: 0.3523 - val_accuracy: 0.8972\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.1498 - accuracy: 0.9539 - val_loss: 0.3672 - val_accuracy: 0.8952\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.1575 - accuracy: 0.9549 - val_loss: 0.3773 - val_accuracy: 0.8990\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.2685 - accuracy: 0.8980\n",
            "Test acc: 0.898\n"
          ]
        }
      ],
      "source": [
        "text_vectorization.adapt(text_only_train_ds)\n",
        "binary_2gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_2gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_2gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "\n",
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"binary_2gram.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(binary_2gram_train_ds.cache(),\n",
        "          validation_data=binary_2gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "model = keras.models.load_model(\"binary_2gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The result of Test accuracy increase to 89.8%.\n",
        "- This can be seen local order is simply important"
      ],
      "metadata": {
        "id": "uXftJ_oZmFDY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doL_QqHwVGHX"
      },
      "source": [
        "#### Bigrams with TF-IDF encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TF-IDF** stands for **“term frequency, inverse document frequency.”**\n",
        "\n",
        "- Adding a bit more information to this representation by ***counting how many times each word or N-gram occurs***, that, by taking the histogram of the words over the text\n",
        "- Count bigram occurrences\n",
        "    \n",
        "    **{\"the\": 2, \"the cat\": 1, \"cat\": 1, \"cat sat\": 1, \"sat\": 1,\n",
        "    \"sat on\": 1, \"on\": 1, \"on the\": 1, \"the mat: 1\", \"mat\": 1}**\n",
        "    \n",
        "- TF-IDF is a metric that fuses these two ideas ***(the terms that appear in almost every document (like “the” or “a”) aren’t particularly informative, while terms that appear only in a small subset of all texts (like “Herzog”) are very distinctive, and thus important.)***.\n",
        "- **TF-IDF normalization** weights a given term by taking ***“term frequency,”*** how many times the term appears in the current document, and dividing it by a measure of ***“document frequency,”*** which estimates how often the term comes up across the dataset."
      ],
      "metadata": {
        "id": "dKbX2TJ42rKS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y19S2BLbVGHY"
      },
      "source": [
        "**Configuring the `TextVectorization` layer to return token counts**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "hXh9n7EWVGHY"
      },
      "outputs": [],
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    ngrams=2,\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"count\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwIPwXlSVGHZ"
      },
      "source": [
        "**Configuring `TextVectorization` to return TF-IDF-weighted outputs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "zlHMzsGuVGHZ"
      },
      "outputs": [],
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    ngrams=2,\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"tf_idf\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPmwfld9VGHa"
      },
      "source": [
        "**Training and testing the TF-IDF bigram model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "sNkR0u33VGHa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e894ae3-6c5a-490c-9d10-b8f230a9f818"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320033 (1.22 MB)\n",
            "Trainable params: 320033 (1.22 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 7s 11ms/step - loss: 0.4847 - accuracy: 0.7735 - val_loss: 0.2957 - val_accuracy: 0.8870\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.3190 - accuracy: 0.8737 - val_loss: 0.3418 - val_accuracy: 0.8620\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2800 - accuracy: 0.8867 - val_loss: 0.3132 - val_accuracy: 0.8672\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2696 - accuracy: 0.8910 - val_loss: 0.3096 - val_accuracy: 0.8818\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2519 - accuracy: 0.9003 - val_loss: 0.3190 - val_accuracy: 0.8754\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2490 - accuracy: 0.9021 - val_loss: 0.3468 - val_accuracy: 0.8678\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2475 - accuracy: 0.9044 - val_loss: 0.3321 - val_accuracy: 0.8694\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2366 - accuracy: 0.9054 - val_loss: 0.3474 - val_accuracy: 0.8694\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2319 - accuracy: 0.9069 - val_loss: 0.3503 - val_accuracy: 0.8632\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2425 - accuracy: 0.9006 - val_loss: 0.3641 - val_accuracy: 0.8522\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 0.3041 - accuracy: 0.8833\n",
            "Test acc: 0.883\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.3041 - accuracy: 0.8833\n",
            "The model gets: 0.883 test accuracy on the IMDB classification task\n"
          ]
        }
      ],
      "source": [
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "tfidf_2gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "tfidf_2gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "tfidf_2gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "\n",
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"tfidf_2gram.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(tfidf_2gram_train_ds.cache(),\n",
        "          validation_data=tfidf_2gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "model = keras.models.load_model(\"tfidf_2gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")\n",
        "print(f\"The model gets: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f} test accuracy on the IMDB classification task\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Inference**"
      ],
      "metadata": {
        "id": "4gOIG5s2DqUE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "lglhomGmVGHb"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
        "processed_inputs = text_vectorization(inputs)\n",
        "outputs = model(processed_inputs)\n",
        "inference_model = keras.Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "M2Io4jNSVGHc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6beb4109-1989-43e7-a5b2-b2b6b7e2a859"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "74.44 percent positive\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "raw_text_data = tf.convert_to_tensor([\n",
        "    [\"That was an excellent movie, I love it.\"],\n",
        "])\n",
        "predictions = inference_model(raw_text_data)\n",
        "print(f\"{float(predictions[0] * 100):.2f} percent positive\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}